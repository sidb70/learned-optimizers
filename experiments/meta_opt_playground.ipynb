{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "\n",
    "        seed = torch.randint(0, 1000, (1,)).item()\n",
    "        init_weights(self.fc1, seed)\n",
    "        init_weights(self.fc2, seed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "class MetaModel(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super(MetaModel, self).__init__()\n",
    "        self.d = d\n",
    "        self.div = d//8\n",
    "        self.fc1 = nn.Linear(d//self.div, 8)\n",
    "        self.fc2 = nn.Linear(8, d)\n",
    "    \n",
    "    def forward(self, theta_flat):\n",
    "        outs = []\n",
    "        if len(theta_flat.shape)==1:\n",
    "            theta_flat = theta_flat.unsqueeze(0)\n",
    "        for i in range(0, len(theta_flat), self.d//self.div):\n",
    "            x = theta_flat[:,i:i+self.d//self.div]\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            outs.append(self.fc2(x))\n",
    "        return torch.cat(outs, dim=0)\n",
    "\n",
    "    \n",
    "def init_weights(module, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    # init weights with xavier \n",
    "    if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        nn.init.zeros_(module.bias)\n",
    "    #print(f\"Init weights with seed {seed}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dim = 1\n",
    "hidden_dim = 32\n",
    "output_dim = 1\n",
    "d = sum(p.numel() for p in BaseModel(input_dim, hidden_dim, output_dim).parameters()) +1\n",
    "\n",
    "n=1000\n",
    "X = torch.rand(n, input_dim)*4 -2\n",
    "Y = torch.tanh(X)\n",
    "# shuffle\n",
    "perm = torch.randperm(n)\n",
    "X = X[perm]\n",
    "Y = Y[perm]\n",
    "# random split\n",
    "X_train, X_test = X[:int(n*0.8)], X[int(n*0.8):]\n",
    "Y_train, Y_test = Y[:int(n*0.8)], Y[int(n*0.8):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss 0.5734468102455139\n",
      "\n",
      "Epoch 1, Train Loss 0.3561665415763855\n",
      "\n",
      "Epoch 2, Train Loss 0.20541901886463165\n",
      "\n",
      "Epoch 3, Train Loss 0.10933011770248413\n",
      "\n",
      "Epoch 4, Train Loss 0.05510542914271355\n",
      "\n",
      "Epoch 5, Train Loss 0.033849503844976425\n",
      "\n",
      "Epoch 6, Train Loss 0.03520331531763077\n",
      "\n",
      "Epoch 7, Train Loss 0.04890383780002594\n",
      "\n",
      "Epoch 8, Train Loss 0.0646982416510582\n",
      "\n",
      "Epoch 9, Train Loss 0.07271748781204224\n",
      "\n",
      "Epoch 10, Train Loss 0.06800239533185959\n",
      "\n",
      "Epoch 11, Train Loss 0.05400583893060684\n",
      "\n",
      "Epoch 12, Train Loss 0.037377454340457916\n",
      "\n",
      "Epoch 13, Train Loss 0.023532535880804062\n",
      "\n",
      "Epoch 14, Train Loss 0.015263435430824757\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 model\n",
    "theta_f = BaseModel(input_dim, hidden_dim, output_dim)\n",
    "meta_model = MetaModel(d)\n",
    "optimizer = optim.Adam(meta_model.parameters(), lr=1e-2)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(15):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # what if we reinit on each iteration? ie can meta model predict final weights from scratch\n",
    "    # theta_f = BaseModel(input_dim, hidden_dim, output_dim)  \n",
    "    \n",
    "    theta_flat = torch.cat([p.flatten() for p in theta_f.parameters()]).requires_grad_(True)\n",
    "    theta_flat = torch.cat([theta_flat, torch.tensor([epoch], dtype=torch.float32)]).requires_grad_(True)\n",
    "    \n",
    "    theta_flat_prime = meta_model(theta_flat).squeeze(0)\n",
    "    \n",
    "    theta_f_prime = BaseModel(input_dim, hidden_dim, output_dim)\n",
    "    \n",
    "    params_dict = {}\n",
    "    start_idx = 0\n",
    "    for name, param in theta_f_prime.named_parameters():\n",
    "        param_length = param.numel()\n",
    "        params_dict[name] = theta_flat_prime[start_idx:start_idx + param_length].view_as(param)\n",
    "        start_idx += param_length\n",
    "    \n",
    "    def modified_forward(x):\n",
    "        # directly pass the weights into the forward pass. keeps the computation graph intact\n",
    "        x = F.linear(x, \n",
    "                    weight=params_dict['fc1.weight'],\n",
    "                    bias=params_dict['fc1.bias'])\n",
    "        x = torch.relu(x)\n",
    "        x = F.linear(x,\n",
    "                    weight=params_dict['fc2.weight'],\n",
    "                    bias=params_dict['fc2.bias'])\n",
    "        return x\n",
    "    \n",
    "    outputs = modified_forward(X_train)\n",
    "    loss = criterion(outputs, Y_train)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch}, Train Loss {loss.item()}\\n')\n",
    "    \n",
    "    if epoch == 0:\n",
    "        make_dot(loss, params=dict(list(meta_model.named_parameters()))).render('comp_graph', format='png')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizee params: 97, Meta model params: 954\n"
     ]
    }
   ],
   "source": [
    "# numel\n",
    "optimizee_params = sum(p.numel() for p in BaseModel(input_dim, hidden_dim, output_dim).parameters())\n",
    "meta_model_params = sum(p.numel() for p in meta_model.parameters())\n",
    "\n",
    "print(f\"Optimizee params: {optimizee_params}, Meta model params: {meta_model_params}\") # currently meta_model >> optimizee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss 0.05349240155518055\n",
      "Epoch 1, Train Loss 0.0033467754647135734\n",
      "Epoch 2, Train Loss 0.000426995464367792\n",
      "Epoch 3, Train Loss 0.00013714719488052652\n",
      "Epoch 4, Train Loss 5.154517467599362e-05\n",
      "Epoch 5, Train Loss 2.351767678919714e-05\n",
      "Epoch 6, Train Loss 2.5258442969061433e-05\n",
      "Epoch 7, Train Loss 7.34056506480556e-05\n",
      "Epoch 8, Train Loss 1.9356284727109595e-05\n",
      "Epoch 9, Train Loss 1.4436008015763946e-05\n",
      "Epoch 10, Train Loss 5.8098170542507435e-05\n",
      "Epoch 11, Train Loss 2.8191078672534787e-05\n",
      "Epoch 12, Train Loss 4.9805582952103577e-05\n",
      "Epoch 13, Train Loss 3.6469967511948195e-05\n",
      "Epoch 14, Train Loss 2.4815549826598725e-05\n",
      "Epoch 15, Train Loss 6.224983045831322e-05\n",
      "Epoch 16, Train Loss 2.2746785660274328e-05\n",
      "Epoch 17, Train Loss 3.850803919340251e-05\n",
      "Epoch 18, Train Loss 3.5103803005767985e-05\n",
      "Epoch 19, Train Loss 4.7738420733367094e-05\n",
      "Epoch 20, Train Loss 4.187559449201217e-05\n",
      "Epoch 21, Train Loss 5.028922976634931e-05\n",
      "Epoch 22, Train Loss 3.762023051240248e-05\n",
      "Epoch 23, Train Loss 2.588235241637449e-05\n",
      "Epoch 24, Train Loss 1.2665792211919325e-05\n"
     ]
    }
   ],
   "source": [
    "#k models\n",
    "k = 500\n",
    "batch_size = 8\n",
    "optimizees = [BaseModel(input_dim, hidden_dim, output_dim) for _ in range(k)]\n",
    "meta_model = MetaModel(d).to('cuda')\n",
    "optimizer = optim.Adam(meta_model.parameters(), lr=1e-2)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "X_train = X_train.to('cuda')\n",
    "Y_train = Y_train.to('cuda')\n",
    "\n",
    "for epoch in range(25):\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, k, batch_size):\n",
    "        batch = optimizees[i:i+batch_size]\n",
    "        batch_flattened = [torch.cat([torch.cat([p.flatten() for p in model.parameters()]), torch.tensor([epoch], dtype=torch.float32)]) for model in batch]\n",
    "        batch_flattened = torch.stack(batch_flattened).to('cuda')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        theta_flat_prime = meta_model(batch_flattened)\n",
    "        \n",
    "        batch_prime = [BaseModel(input_dim, hidden_dim, output_dim) for _ in range(theta_flat_prime.shape[0])]\n",
    "        params_dict = {}\n",
    "        start_idx = 0\n",
    "        for name, param in batch_prime[0].named_parameters():\n",
    "            param_length = param.numel() \n",
    "            try:\n",
    "                params_dict[name] = theta_flat_prime[:, start_idx:start_idx + param_length].view(theta_flat_prime.shape[0], *param.shape)\n",
    "            except:\n",
    "                print(name, param_length, theta_flat_prime[:, start_idx:start_idx + param_length].shape)\n",
    "                break\n",
    "            start_idx += param_length\n",
    "\n",
    "        def modified_forward(x, idx):\n",
    "            # directly pass the weights\n",
    "            x = F.linear(x, \n",
    "                        weight=params_dict['fc1.weight'][idx],\n",
    "                        bias=params_dict['fc1.bias'][idx])\n",
    "            x = torch.relu(x)\n",
    "            x = F.linear(x,\n",
    "                        weight=params_dict['fc2.weight'][idx],\n",
    "                        bias=params_dict['fc2.bias'][idx])\n",
    "            return x\n",
    "        \n",
    "        \n",
    "        outputs = [modified_forward(X_train, idx) for idx in range(len(batch_prime))]\n",
    "        # reassign the outputs to the models\n",
    "        for i, model in enumerate(optimizees[i:i+batch_size]):\n",
    "            for name, param in model.named_parameters():\n",
    "                param.data = params_dict[name][i].cpu()\n",
    "        loss = torch.stack([criterion(output, Y_train) for output in outputs]).mean()\n",
    "        loss.backward()\n",
    "        running_loss += loss.item() * len(batch)\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch}, Train Loss {running_loss/len(optimizees)}')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, eval Train Loss 0.017968200147151947\n",
      "Epoch 1, eval Train Loss 0.00040609604911878705\n",
      "Epoch 2, eval Train Loss 1.856899689300917e-05\n",
      "Epoch 3, eval Train Loss 7.199543688329868e-06\n",
      "Epoch 4, eval Train Loss 6.9732332121930085e-06\n",
      "Epoch 5, eval Train Loss 6.981742444622796e-06\n",
      "Epoch 6, eval Train Loss 6.981813385209534e-06\n",
      "Epoch 7, eval Train Loss 6.9812399488000665e-06\n",
      "Epoch 8, eval Train Loss 6.981141268624924e-06\n",
      "Epoch 9, eval Train Loss 6.981146725593135e-06\n",
      "Epoch 10, eval Train Loss 6.981133083172608e-06\n",
      "Epoch 11, eval Train Loss 6.98111853125738e-06\n",
      "Epoch 12, eval Train Loss 6.981108072068309e-06\n",
      "Epoch 13, eval Train Loss 6.981108072068309e-06\n",
      "Epoch 14, eval Train Loss 6.981108072068309e-06\n",
      "Epoch 15, eval Train Loss 6.981108072068309e-06\n",
      "Epoch 16, eval Train Loss 6.981108072068309e-06\n",
      "Epoch 17, eval Train Loss 6.981108072068309e-06\n",
      "Epoch 18, eval Train Loss 6.981108072068309e-06\n",
      "Epoch 19, eval Train Loss 6.981108072068309e-06\n",
      "Epoch 20, eval Train Loss 6.981108072068309e-06\n",
      "Epoch 21, eval Train Loss 6.981108072068309e-06\n",
      "Epoch 22, eval Train Loss 6.981108072068309e-06\n",
      "Epoch 23, eval Train Loss 6.981108072068309e-06\n",
      "Epoch 24, eval Train Loss 6.981108072068309e-06\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n",
      "Epoch 24, Adam Train Loss 0.018073348328471184\n",
      "Epoch 24, Adam Train Loss 0.08623751252889633\n",
      "Epoch 24, Adam Train Loss 0.037158019840717316\n",
      "Epoch 24, Adam Train Loss 0.017903659492731094\n"
     ]
    }
   ],
   "source": [
    "# evaluate overparameterized meta optimizer\n",
    "k_test = 100\n",
    "batch_size = 8\n",
    "optimizees = [BaseModel(input_dim, hidden_dim, output_dim) for _ in range(k_test)]\n",
    "optimizees_for_adam = [deepcopy(model) for model in optimizees]\n",
    "meta_model.eval()\n",
    "X_train = X_train.to('cuda')\n",
    "Y_train = Y_train.to('cuda')\n",
    "\n",
    "avg_losses_meta = []\n",
    "for epoch in range(25):\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, k_test, batch_size):\n",
    "        batch = optimizees[i:i+batch_size]\n",
    "        batch_flattened = [torch.cat([torch.cat([p.flatten() for p in model.parameters()]), torch.tensor([epoch], dtype=torch.float32)]) for model in batch]\n",
    "        batch_flattened = torch.stack(batch_flattened).to('cuda')\n",
    "\n",
    "        theta_flat_prime = meta_model(batch_flattened)\n",
    "        \n",
    "        batch_prime = [BaseModel(input_dim, hidden_dim, output_dim) for _ in range(theta_flat_prime.shape[0])]\n",
    "        params_dict = {}\n",
    "        start_idx = 0\n",
    "        for name, param in batch_prime[0].named_parameters():\n",
    "            param_length = param.numel() \n",
    "            params_dict[name] = theta_flat_prime[:, start_idx:start_idx + param_length].view(theta_flat_prime.shape[0], *param.shape)\n",
    "            \n",
    "            start_idx += param_length\n",
    "\n",
    "        def modified_forward(x, idx):\n",
    "            # directly pass the weights\n",
    "            x = F.linear(x, \n",
    "                        weight=params_dict['fc1.weight'][idx],\n",
    "                        bias=params_dict['fc1.bias'][idx])\n",
    "            x = torch.relu(x)\n",
    "            x = F.linear(x,\n",
    "                        weight=params_dict['fc2.weight'][idx],\n",
    "                        bias=params_dict['fc2.bias'][idx])\n",
    "            return x\n",
    "        \n",
    "        \n",
    "        outputs = [modified_forward(X_train, idx) for idx in range(len(batch_prime))]\n",
    "        # reassign the weights for the next iteration\n",
    "        for i, model in enumerate(batch):\n",
    "            for name, param in model.named_parameters():\n",
    "                param.data = params_dict[name][i].data.cpu()\n",
    "\n",
    "        loss = torch.stack([criterion(output, Y_train) for output in outputs]).mean()\n",
    "        running_loss += loss.item() * len(batch)\n",
    "    avg_losses_meta.append(running_loss/len(optimizees))\n",
    "    print(f'Epoch {epoch}, eval Train Loss {running_loss/len(optimizees)}')\n",
    "\n",
    "\n",
    "avg_losses_adam = [[] for _ in range(25)]\n",
    "for model in optimizees_for_adam:\n",
    "    model.to('cuda')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "    for epoch in range(25):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, Y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_losses_adam[epoch].append(loss.item())\n",
    "    print(f'Epoch {epoch}, Adam Train Loss {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plto\n",
    "\n",
    "avg_losses_adam = [sum(losses)/len(losses) for losses in avg_losses_adam]\n",
    "\n",
    "plt.plot(avg_losses_meta, label='Meta')\n",
    "plt.plot(avg_losses_adam, label='Adam')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 0.32124263048171997\n",
      "\n",
      "Epoch 1, Loss 0.30983981490135193\n",
      "\n",
      "Epoch 2, Loss 0.2987545430660248\n",
      "\n",
      "Epoch 3, Loss 0.28798747062683105\n",
      "\n",
      "Epoch 4, Loss 0.2775389850139618\n",
      "\n",
      "Epoch 5, Loss 0.26740792393684387\n",
      "\n",
      "Epoch 6, Loss 0.25759318470954895\n",
      "\n",
      "Epoch 7, Loss 0.24809294939041138\n",
      "\n",
      "Epoch 8, Loss 0.23890462517738342\n",
      "\n",
      "Epoch 9, Loss 0.23002487421035767\n",
      "\n",
      "Epoch 10, Loss 0.22144976258277893\n",
      "\n",
      "Epoch 11, Loss 0.21317480504512787\n",
      "\n",
      "Epoch 12, Loss 0.20519483089447021\n",
      "\n",
      "Epoch 13, Loss 0.19750425219535828\n",
      "\n",
      "Epoch 14, Loss 0.1900968700647354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "model = BaseModel(input_dim, hidden_dim, output_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "for epoch in range(15):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, Y)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch}, Loss {loss.item()}\\n')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
