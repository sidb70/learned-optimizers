{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BaseModel, self).__init__()\n",
    "        seed = torch.randint(0, 1000, (1,)).item()\n",
    "        torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        init_weights(self.fc1, seed)\n",
    "        init_weights(self.fc2, seed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "class MetaModel(nn.Module):\n",
    "    def __init__(self, optimizee_in_dim, meta_in_dim=8):\n",
    "        super(MetaModel, self).__init__()\n",
    "        self.d = optimizee_in_dim\n",
    "        self.div = optimizee_in_dim//meta_in_dim\n",
    "        self.fc1 = nn.Linear(optimizee_in_dim//self.div, meta_in_dim)\n",
    "        self.fc2 = nn.Linear(meta_in_dim, optimizee_in_dim)\n",
    "    \n",
    "    def forward(self, theta_flat):\n",
    "        outs = []\n",
    "        if len(theta_flat.shape)==1:\n",
    "            theta_flat = theta_flat.unsqueeze(0)\n",
    "        for i in range(0, len(theta_flat), self.d//self.div):\n",
    "            x = theta_flat[:,i:i+self.d//self.div]\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            outs.append(self.fc2(x))\n",
    "        return torch.cat(outs, dim=0)\n",
    "\n",
    "    \n",
    "def init_weights(module, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    # init weights with xavier \n",
    "    # if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
    "    #     nn.init.xavier_uniform_(module.weight)\n",
    "    #     nn.init.zeros_(module.bias)\n",
    "\n",
    "    #print(f\"Init weights with seed {seed}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dim = 1\n",
    "hidden_dim = 32\n",
    "output_dim = 1\n",
    "d = sum(p.numel() for p in BaseModel(input_dim, hidden_dim, output_dim).parameters()) +1\n",
    "\n",
    "n=1000\n",
    "X = torch.rand(n, input_dim)*4 -2\n",
    "Y = torch.tanh(X)\n",
    "# shuffle\n",
    "perm = torch.randperm(n)\n",
    "X = X[perm]\n",
    "Y = Y[perm]\n",
    "# random split\n",
    "X_train, X_test = X[:int(n*0.8)], X[int(n*0.8):]\n",
    "Y_train, Y_test = Y[:int(n*0.8)], Y[int(n*0.8):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss 0.33560967445373535\n",
      "\n",
      "Epoch 1, Train Loss 0.03629552200436592\n",
      "\n",
      "Epoch 2, Train Loss 0.09167639911174774\n",
      "\n",
      "Epoch 3, Train Loss 0.07882853597402573\n",
      "\n",
      "Epoch 4, Train Loss 0.049331773072481155\n",
      "\n",
      "Epoch 5, Train Loss 0.038694243878126144\n",
      "\n",
      "Epoch 6, Train Loss 0.030783681198954582\n",
      "\n",
      "Epoch 7, Train Loss 0.022609837353229523\n",
      "\n",
      "Epoch 8, Train Loss 0.019743341952562332\n",
      "\n",
      "Epoch 9, Train Loss 0.023093603551387787\n",
      "\n",
      "Epoch 10, Train Loss 0.02837788127362728\n",
      "\n",
      "Epoch 11, Train Loss 0.030902421101927757\n",
      "\n",
      "Epoch 12, Train Loss 0.028349943459033966\n",
      "\n",
      "Epoch 13, Train Loss 0.0214324239641428\n",
      "\n",
      "Epoch 14, Train Loss 0.013026557862758636\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 model\n",
    "theta_f = BaseModel(input_dim, hidden_dim, output_dim)\n",
    "meta_model = MetaModel(d)\n",
    "optimizer = optim.Adam(meta_model.parameters(), lr=1e-2)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(15):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # what if we reinit on each iteration? ie can meta model predict final weights from scratch\n",
    "    # theta_f = BaseModel(input_dim, hidden_dim, output_dim)  \n",
    "    \n",
    "    theta_flat = torch.cat([p.flatten() for p in theta_f.parameters()]).requires_grad_(True)\n",
    "    theta_flat = torch.cat([theta_flat, torch.tensor([epoch], dtype=torch.float32)]).requires_grad_(True)\n",
    "    \n",
    "    theta_flat_prime = meta_model(theta_flat).squeeze(0)\n",
    "    \n",
    "    theta_f_prime = BaseModel(input_dim, hidden_dim, output_dim)\n",
    "    \n",
    "    params_dict = {}\n",
    "    start_idx = 0\n",
    "    for name, param in theta_f_prime.named_parameters():\n",
    "        param_length = param.numel()\n",
    "        params_dict[name] = theta_flat_prime[start_idx:start_idx + param_length].view_as(param)\n",
    "        start_idx += param_length\n",
    "    \n",
    "    def modified_forward(x):\n",
    "        # directly pass the weights into the forward pass. keeps the computation graph intact\n",
    "        x = F.linear(x, \n",
    "                    weight=params_dict['fc1.weight'],\n",
    "                    bias=params_dict['fc1.bias'])\n",
    "        x = torch.relu(x)\n",
    "        x = F.linear(x,\n",
    "                    weight=params_dict['fc2.weight'],\n",
    "                    bias=params_dict['fc2.bias'])\n",
    "        return x\n",
    "    \n",
    "    outputs = modified_forward(X_train)\n",
    "    loss = criterion(outputs, Y_train)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch}, Train Loss {loss.item()}\\n')\n",
    "    \n",
    "    if epoch == 0:\n",
    "        make_dot(loss, params=dict(list(meta_model.named_parameters()))).render('comp_graph', format='png')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizee params: 97, Meta model params: 954\n"
     ]
    }
   ],
   "source": [
    "# numel\n",
    "optimizee_params = sum(p.numel() for p in BaseModel(input_dim, hidden_dim, output_dim).parameters())\n",
    "meta_model_params = sum(p.numel() for p in meta_model.parameters())\n",
    "\n",
    "print(f\"Optimizee params: {optimizee_params}, Meta model params: {meta_model_params}\") # currently meta_model >> optimizee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss 0.06858942236751318\n",
      "Epoch 1, Train Loss 0.0017153289506677538\n",
      "Epoch 2, Train Loss 0.00025893434800673275\n",
      "Epoch 3, Train Loss 0.00012807434413116426\n",
      "Epoch 4, Train Loss 6.596780658583157e-05\n",
      "Epoch 5, Train Loss 3.462421795120463e-05\n",
      "Epoch 6, Train Loss 1.811641452513868e-05\n",
      "Epoch 7, Train Loss 1.2784575883415528e-05\n",
      "Epoch 8, Train Loss 1.0806434365804307e-05\n",
      "Epoch 9, Train Loss 9.764624133822509e-06\n",
      "Epoch 10, Train Loss 9.182950147078372e-06\n",
      "Epoch 11, Train Loss 8.798366397968494e-06\n",
      "Epoch 12, Train Loss 8.537405265087727e-06\n",
      "Epoch 13, Train Loss 8.326828676217701e-06\n",
      "Epoch 14, Train Loss 8.125856817059684e-06\n",
      "Epoch 15, Train Loss 7.922293436422479e-06\n",
      "Epoch 16, Train Loss 7.73116220079828e-06\n",
      "Epoch 17, Train Loss 7.545303720689844e-06\n",
      "Epoch 18, Train Loss 7.347187503910391e-06\n",
      "Epoch 19, Train Loss 7.130973677703878e-06\n",
      "Epoch 20, Train Loss 6.944780798221473e-06\n",
      "Epoch 21, Train Loss 6.7798099116771486e-06\n",
      "Epoch 22, Train Loss 6.636102585616755e-06\n",
      "Epoch 23, Train Loss 6.504642304207664e-06\n",
      "Epoch 24, Train Loss 6.381250572303543e-06\n"
     ]
    }
   ],
   "source": [
    "#k models\n",
    "k = 500\n",
    "batch_size = 8\n",
    "optimizees = [BaseModel(input_dim, hidden_dim, output_dim) for _ in range(k)]\n",
    "meta_model = MetaModel(d).to('cuda')\n",
    "optimizer = optim.Adam(meta_model.parameters(), lr=1e-2)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "X_train = X_train.to('cuda')\n",
    "Y_train = Y_train.to('cuda')\n",
    "\n",
    "for epoch in range(25):\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, k, batch_size):\n",
    "        batch = optimizees[i:i+batch_size]\n",
    "        batch_flattened = [torch.cat([torch.cat([p.flatten() for p in model.parameters()]), torch.tensor([epoch], dtype=torch.float32)]) for model in batch]\n",
    "        batch_flattened = torch.stack(batch_flattened).to('cuda')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        theta_flat_prime = meta_model(batch_flattened)\n",
    "        \n",
    "        batch_prime = [BaseModel(input_dim, hidden_dim, output_dim) for _ in range(theta_flat_prime.shape[0])]\n",
    "        params_dict = {}\n",
    "        start_idx = 0\n",
    "        for name, param in batch_prime[0].named_parameters():\n",
    "            param_length = param.numel() \n",
    "            try:\n",
    "                params_dict[name] = theta_flat_prime[:, start_idx:start_idx + param_length].view(theta_flat_prime.shape[0], *param.shape)\n",
    "            except:\n",
    "                print(name, param_length, theta_flat_prime[:, start_idx:start_idx + param_length].shape)\n",
    "                break\n",
    "            start_idx += param_length\n",
    "\n",
    "        def modified_forward(x, idx):\n",
    "            # directly pass the weights\n",
    "            x = F.linear(x, \n",
    "                        weight=params_dict['fc1.weight'][idx],\n",
    "                        bias=params_dict['fc1.bias'][idx])\n",
    "            x = torch.relu(x)\n",
    "            x = F.linear(x,\n",
    "                        weight=params_dict['fc2.weight'][idx],\n",
    "                        bias=params_dict['fc2.bias'][idx])\n",
    "            return x\n",
    "        \n",
    "        \n",
    "        outputs = [modified_forward(X_train, idx) for idx in range(len(batch_prime))]\n",
    "        # reassign the outputs to the models\n",
    "        for i, model in enumerate(optimizees[i:i+batch_size]):\n",
    "            for name, param in model.named_parameters():\n",
    "                param.data = params_dict[name][i].cpu()\n",
    "        loss = torch.stack([criterion(output, Y_train) for output in outputs]).mean()\n",
    "        loss.backward()\n",
    "        running_loss += loss.item() * len(batch)\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch}, Train Loss {running_loss/len(optimizees)}')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Train Loss 0.6034880417585373\n",
      "Epoch 0, eval Train Loss 0.006960599422454834\n",
      "Epoch 1, eval Train Loss 0.0004154448735062033\n",
      "Epoch 2, eval Train Loss 1.87337821625988e-05\n",
      "Epoch 3, eval Train Loss 6.644122040597722e-06\n",
      "Epoch 4, eval Train Loss 6.338917337416206e-06\n",
      "Epoch 5, eval Train Loss 6.329472944344161e-06\n",
      "Epoch 6, eval Train Loss 6.329170882963809e-06\n",
      "Epoch 7, eval Train Loss 6.329400330287171e-06\n",
      "Epoch 8, eval Train Loss 6.329441930574831e-06\n",
      "Epoch 9, eval Train Loss 6.3294376377598385e-06\n",
      "Epoch 10, eval Train Loss 6.32943430900923e-06\n",
      "Epoch 11, eval Train Loss 6.329444113362115e-06\n",
      "Epoch 12, eval Train Loss 6.329453572107013e-06\n",
      "Epoch 13, eval Train Loss 6.32945495453896e-06\n",
      "Epoch 14, eval Train Loss 6.329455736704403e-06\n",
      "Epoch 15, eval Train Loss 6.329455209197476e-06\n",
      "Epoch 16, eval Train Loss 6.329455736704403e-06\n",
      "Epoch 17, eval Train Loss 6.329455209197476e-06\n",
      "Epoch 18, eval Train Loss 6.329455736704403e-06\n",
      "Epoch 19, eval Train Loss 6.329455209197476e-06\n",
      "Epoch 20, eval Train Loss 6.329455736704403e-06\n",
      "Epoch 21, eval Train Loss 6.329455209197476e-06\n",
      "Epoch 22, eval Train Loss 6.329455736704403e-06\n",
      "Epoch 23, eval Train Loss 6.329455209197476e-06\n",
      "Epoch 24, eval Train Loss 6.329455736704403e-06\n",
      "Epoch 24, Adam Train Loss 0.024858444929122925\n",
      "Epoch 24, Adam Train Loss 0.015883058309555054\n",
      "Epoch 24, Adam Train Loss 0.010544911026954651\n",
      "Epoch 24, Adam Train Loss 0.017175132408738136\n",
      "Epoch 24, Adam Train Loss 0.01568196527659893\n",
      "Epoch 24, Adam Train Loss 0.007946042343974113\n",
      "Epoch 24, Adam Train Loss 0.013826076872646809\n",
      "Epoch 24, Adam Train Loss 0.03100978396832943\n",
      "Epoch 24, Adam Train Loss 0.007458964828401804\n",
      "Epoch 24, Adam Train Loss 0.005635467823594809\n",
      "Epoch 24, Adam Train Loss 0.012363729067146778\n",
      "Epoch 24, Adam Train Loss 0.0057098232209682465\n",
      "Epoch 24, Adam Train Loss 0.013772670179605484\n",
      "Epoch 24, Adam Train Loss 0.012158200144767761\n",
      "Epoch 24, Adam Train Loss 0.03629092499613762\n",
      "Epoch 24, Adam Train Loss 0.06520719081163406\n",
      "Epoch 24, Adam Train Loss 0.022789735347032547\n",
      "Epoch 24, Adam Train Loss 0.020525522530078888\n",
      "Epoch 24, Adam Train Loss 0.024485759437084198\n",
      "Epoch 24, Adam Train Loss 0.022735361009836197\n",
      "Epoch 24, Adam Train Loss 0.005639327690005302\n",
      "Epoch 24, Adam Train Loss 0.008798858150839806\n",
      "Epoch 24, Adam Train Loss 0.0033950733486562967\n",
      "Epoch 24, Adam Train Loss 0.02300945296883583\n",
      "Epoch 24, Adam Train Loss 0.02722044847905636\n",
      "Epoch 24, Adam Train Loss 0.024858444929122925\n",
      "Epoch 24, Adam Train Loss 0.015883058309555054\n",
      "Epoch 24, Adam Train Loss 0.010544911026954651\n",
      "Epoch 24, Adam Train Loss 0.017175132408738136\n",
      "Epoch 24, Adam Train Loss 0.01568196527659893\n",
      "Epoch 24, Adam Train Loss 0.007946042343974113\n",
      "Epoch 24, Adam Train Loss 0.013826076872646809\n",
      "Epoch 24, Adam Train Loss 0.03100978396832943\n",
      "Epoch 24, Adam Train Loss 0.007458964828401804\n",
      "Epoch 24, Adam Train Loss 0.005635467823594809\n",
      "Epoch 24, Adam Train Loss 0.012363729067146778\n",
      "Epoch 24, Adam Train Loss 0.0057098232209682465\n",
      "Epoch 24, Adam Train Loss 0.013772670179605484\n",
      "Epoch 24, Adam Train Loss 0.012158200144767761\n",
      "Epoch 24, Adam Train Loss 0.03629092499613762\n",
      "Epoch 24, Adam Train Loss 0.06520719081163406\n",
      "Epoch 24, Adam Train Loss 0.022789735347032547\n",
      "Epoch 24, Adam Train Loss 0.020525522530078888\n",
      "Epoch 24, Adam Train Loss 0.024485759437084198\n",
      "Epoch 24, Adam Train Loss 0.022735361009836197\n",
      "Epoch 24, Adam Train Loss 0.005639327690005302\n",
      "Epoch 24, Adam Train Loss 0.008798858150839806\n",
      "Epoch 24, Adam Train Loss 0.0033950733486562967\n",
      "Epoch 24, Adam Train Loss 0.02300945296883583\n",
      "Epoch 24, Adam Train Loss 0.02722044847905636\n",
      "Epoch 24, Adam Train Loss 0.024858444929122925\n",
      "Epoch 24, Adam Train Loss 0.015883058309555054\n",
      "Epoch 24, Adam Train Loss 0.010544911026954651\n",
      "Epoch 24, Adam Train Loss 0.017175132408738136\n",
      "Epoch 24, Adam Train Loss 0.01568196527659893\n",
      "Epoch 24, Adam Train Loss 0.007946042343974113\n",
      "Epoch 24, Adam Train Loss 0.013826076872646809\n",
      "Epoch 24, Adam Train Loss 0.03100978396832943\n",
      "Epoch 24, Adam Train Loss 0.007458964828401804\n",
      "Epoch 24, Adam Train Loss 0.005635467823594809\n",
      "Epoch 24, Adam Train Loss 0.012363729067146778\n",
      "Epoch 24, Adam Train Loss 0.0057098232209682465\n",
      "Epoch 24, Adam Train Loss 0.013772670179605484\n",
      "Epoch 24, Adam Train Loss 0.012158200144767761\n",
      "Epoch 24, Adam Train Loss 0.03629092499613762\n",
      "Epoch 24, Adam Train Loss 0.06520719081163406\n",
      "Epoch 24, Adam Train Loss 0.022789735347032547\n",
      "Epoch 24, Adam Train Loss 0.020525522530078888\n",
      "Epoch 24, Adam Train Loss 0.024485759437084198\n",
      "Epoch 24, Adam Train Loss 0.022735361009836197\n",
      "Epoch 24, Adam Train Loss 0.005639327690005302\n",
      "Epoch 24, Adam Train Loss 0.008798858150839806\n",
      "Epoch 24, Adam Train Loss 0.0033950733486562967\n",
      "Epoch 24, Adam Train Loss 0.02300945296883583\n",
      "Epoch 24, Adam Train Loss 0.02722044847905636\n",
      "Epoch 24, Adam Train Loss 0.024858444929122925\n",
      "Epoch 24, Adam Train Loss 0.015883058309555054\n",
      "Epoch 24, Adam Train Loss 0.010544911026954651\n",
      "Epoch 24, Adam Train Loss 0.017175132408738136\n",
      "Epoch 24, Adam Train Loss 0.01568196527659893\n",
      "Epoch 24, Adam Train Loss 0.007946042343974113\n",
      "Epoch 24, Adam Train Loss 0.013826076872646809\n",
      "Epoch 24, Adam Train Loss 0.03100978396832943\n",
      "Epoch 24, Adam Train Loss 0.007458964828401804\n",
      "Epoch 24, Adam Train Loss 0.005635467823594809\n",
      "Epoch 24, Adam Train Loss 0.012363729067146778\n",
      "Epoch 24, Adam Train Loss 0.0057098232209682465\n",
      "Epoch 24, Adam Train Loss 0.013772670179605484\n",
      "Epoch 24, Adam Train Loss 0.012158200144767761\n",
      "Epoch 24, Adam Train Loss 0.03629092499613762\n",
      "Epoch 24, Adam Train Loss 0.06520719081163406\n",
      "Epoch 24, Adam Train Loss 0.022789735347032547\n",
      "Epoch 24, Adam Train Loss 0.020525522530078888\n",
      "Epoch 24, Adam Train Loss 0.024485759437084198\n",
      "Epoch 24, Adam Train Loss 0.022735361009836197\n",
      "Epoch 24, Adam Train Loss 0.005639327690005302\n",
      "Epoch 24, Adam Train Loss 0.008798858150839806\n",
      "Epoch 24, Adam Train Loss 0.0033950733486562967\n",
      "Epoch 24, Adam Train Loss 0.02300945296883583\n",
      "Epoch 24, Adam Train Loss 0.02722044847905636\n"
     ]
    }
   ],
   "source": [
    "# evaluate overparameterized meta optimizer\n",
    "k_test = 100\n",
    "batch_size = 8\n",
    "optimizees = [BaseModel(input_dim, hidden_dim, output_dim) for _ in range(k_test)]\n",
    "optimizees_for_adam = [deepcopy(model) for model in optimizees]\n",
    "meta_model.eval()\n",
    "X_train = X_train.to('cuda')\n",
    "Y_train = Y_train.to('cuda')\n",
    "\n",
    "# initial evaluation on train\n",
    "initial_losses = [] \n",
    "with torch.no_grad():\n",
    "    for model in optimizees:\n",
    "        model.to('cuda')\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, Y_train)\n",
    "        initial_losses.append(loss.item())\n",
    "    print(f'Initial Train Loss {sum(initial_losses)/len(optimizees)}')\n",
    "\n",
    "epochs = 25\n",
    "losses_meta = [[] for _ in range(epochs+1)]\n",
    "losses_meta[0] = initial_losses\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, k_test, batch_size):\n",
    "        batch = optimizees[i:i+batch_size]\n",
    "        batch_flattened = [torch.cat([torch.cat([p.flatten() for p in model.parameters()]), torch.tensor([epoch], dtype=torch.float32).cuda()]) for model in batch]\n",
    "        batch_flattened = torch.stack(batch_flattened).to('cuda')\n",
    "\n",
    "        theta_flat_prime = meta_model(batch_flattened)\n",
    "        \n",
    "        batch_prime = [BaseModel(input_dim, hidden_dim, output_dim) for _ in range(theta_flat_prime.shape[0])]\n",
    "        params_dict = {}\n",
    "        start_idx = 0\n",
    "        for name, param in batch_prime[0].named_parameters():\n",
    "            param_length = param.numel() \n",
    "            params_dict[name] = theta_flat_prime[:, start_idx:start_idx + param_length].view(theta_flat_prime.shape[0], *param.shape)\n",
    "            \n",
    "            start_idx += param_length\n",
    "\n",
    "        def modified_forward(x, idx):\n",
    "            # directly pass the weights\n",
    "            x = F.linear(x, \n",
    "                        weight=params_dict['fc1.weight'][idx],\n",
    "                        bias=params_dict['fc1.bias'][idx])\n",
    "            x = torch.relu(x)\n",
    "            x = F.linear(x,\n",
    "                        weight=params_dict['fc2.weight'][idx],\n",
    "                        bias=params_dict['fc2.bias'][idx])\n",
    "            return x\n",
    "        \n",
    "        \n",
    "        outputs = [modified_forward(X_train, idx) for idx in range(len(batch_prime))]\n",
    "        # reassign the weights for the next iteration\n",
    "        for i, model in enumerate(batch):\n",
    "            for name, param in model.named_parameters():\n",
    "                param.data = params_dict[name][i].data\n",
    "\n",
    "        loss = torch.stack([criterion(output, Y_train) for output in outputs])\n",
    "        losses_meta[epoch+1].extend(loss.clone().detach().flatten().squeeze().cpu().numpy().tolist())\n",
    "        loss = loss.mean()\n",
    "        running_loss += loss.item() * len(batch)\n",
    "    print(f'Epoch {epoch}, eval Train Loss {running_loss/len(optimizees)}')\n",
    "\n",
    "\n",
    "losses_adam = [[] for _ in range(epochs+1)]\n",
    "losses_adam[0] = initial_losses\n",
    "for model in optimizees_for_adam:\n",
    "    model.to('cuda')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, Y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses_adam[epoch+1].append(loss.item())\n",
    "    print(f'Epoch {epoch}, Adam Train Loss {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plto\n",
    "avg_losses_meta = [sum(losses)/len(losses) for losses in losses_meta]\n",
    "avg_losses_adam = [sum(losses)/len(losses) for losses in losses_adam]\n",
    "\n",
    "# error bars\n",
    "std_losses_meta = [torch.tensor(losses).std().item() for losses in losses_meta]\n",
    "std_losses_adam = [torch.tensor(losses).std().item() for losses in losses_adam]\n",
    "\n",
    "plt.errorbar(range(epochs+1), avg_losses_meta, yerr=std_losses_meta, label='Meta')\n",
    "plt.errorbar(range(epochs+1), avg_losses_adam, yerr=std_losses_adam, label='Adam')\n",
    "\n",
    "\n",
    "plt.plot(avg_losses_meta, label='Meta')\n",
    "plt.plot(avg_losses_adam, label='Adam')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 0.32124263048171997\n",
      "\n",
      "Epoch 1, Loss 0.30983981490135193\n",
      "\n",
      "Epoch 2, Loss 0.2987545430660248\n",
      "\n",
      "Epoch 3, Loss 0.28798747062683105\n",
      "\n",
      "Epoch 4, Loss 0.2775389850139618\n",
      "\n",
      "Epoch 5, Loss 0.26740792393684387\n",
      "\n",
      "Epoch 6, Loss 0.25759318470954895\n",
      "\n",
      "Epoch 7, Loss 0.24809294939041138\n",
      "\n",
      "Epoch 8, Loss 0.23890462517738342\n",
      "\n",
      "Epoch 9, Loss 0.23002487421035767\n",
      "\n",
      "Epoch 10, Loss 0.22144976258277893\n",
      "\n",
      "Epoch 11, Loss 0.21317480504512787\n",
      "\n",
      "Epoch 12, Loss 0.20519483089447021\n",
      "\n",
      "Epoch 13, Loss 0.19750425219535828\n",
      "\n",
      "Epoch 14, Loss 0.1900968700647354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "model = BaseModel(input_dim, hidden_dim, output_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "for epoch in range(15):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, Y)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch}, Loss {loss.item()}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mnist\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "new_mirror = 'https://ossci-datasets.s3.amazonaws.com/mnist'\n",
    "torchvision.datasets.MNIST.resources = [\n",
    "   ('/'.join([new_mirror, url.split('/')[-1]]), md5)\n",
    "   for url, md5 in torchvision.datasets.MNIST.resources\n",
    "]\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./.data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./.data', train=False, download=True, transform=transform)   \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True,pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST MLP #params:  55050\n"
     ]
    }
   ],
   "source": [
    "class MNIST_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_MLP, self).__init__()\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(28*28, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)\n",
    "    \n",
    "print(\"MNIST MLP #params: \", sum(p.numel() for p in MNIST_MLP().parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 0.38018541174530984\n",
      "Epoch 1, Loss 0.1990951547374328\n",
      "Epoch 2, Loss 0.15067081221863626\n",
      "Epoch 3, Loss 0.12670108838888505\n",
      "Epoch 4, Loss 0.11281182187298934\n",
      "Epoch 5, Loss 0.10083507514055819\n",
      "Epoch 6, Loss 0.09084022637351105\n",
      "Epoch 7, Loss 0.0835723392388473\n",
      "Epoch 8, Loss 0.07622918983983497\n",
      "Epoch 9, Loss 0.0741366071026617\n",
      "Epoch 10, Loss 0.06670538113856067\n",
      "Epoch 11, Loss 0.06394986311641211\n",
      "Epoch 12, Loss 0.060327415484997135\n",
      "Epoch 13, Loss 0.05808035696696024\n",
      "Epoch 14, Loss 0.05397719268865185\n",
      "Epoch 15, Loss 0.053860319771889285\n",
      "Epoch 16, Loss 0.05091579967882329\n",
      "Epoch 17, Loss 0.04807739330239128\n",
      "Epoch 18, Loss 0.04776675155478976\n",
      "Epoch 19, Loss 0.043056013567087936\n",
      "Epoch 20, Loss 0.04395673650925552\n",
      "Epoch 21, Loss 0.04316002810870802\n",
      "Epoch 22, Loss 0.03895496913833679\n",
      "Epoch 23, Loss 0.039845913266538024\n",
      "Epoch 24, Loss 0.03876570250455164\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model = MNIST_MLP().cuda()\n",
    "# baseline train\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(25):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch}, Loss {running_loss/len(trainloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimzee # params 55050\n",
      "Meta model # params 23548\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimzee # params\", sum(p.numel() for p in MNIST_MLP().parameters()))\n",
    "print(\"Meta model # params\", sum(p.numel() for p in MetaModel(28*28, 28).parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 500\n",
    "batch_size = 8\n",
    "optimizees = [MNIST_MLP() for _ in range(k)]\n",
    "\n",
    "meta_model = MetaModel(28*28, meta_in_dim=28).to('cuda')\n",
    "optimizer = optim.Adam(meta_model.parameters(), lr=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True,pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "initial_losses = []\n",
    "with torch.no_grad():\n",
    "    for model in optimizees:\n",
    "        model.to('cuda')\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to('cuda')\n",
    "            labels = labels.to('cuda')\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "        initial_losses.append(running_loss/len(trainloader))\n",
    "    print(f'Initial Train Loss {sum(initial_losses)/len(optimizees)}')\n",
    "\n",
    "epochs = 25\n",
    "losses_meta = [[] for _ in range(epochs+1)]\n",
    "losses_meta[0] = initial_losses\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, k, batch_size):\n",
    "        batch = optimizees[i:i+batch_size]\n",
    "        batch_flattened = [torch.cat([torch.cat([p.flatten() for p in model.parameters()]), torch.tensor([epoch], dtype=torch.float32).cuda()]) for model in batch]\n",
    "        batch_flattened = torch.stack(batch_flattened).to('cuda')\n",
    "\n",
    "        theta_flat_prime = meta_model(batch_flattened)\n",
    "        \n",
    "        batch_prime = [MNIST_MLP() for _ in range(theta_flat_prime.shape[0])]\n",
    "        params_dict = {}\n",
    "        start_idx = 0\n",
    "        for name, param in batch_prime[0].named_parameters():\n",
    "            param_length = param.numel() \n",
    "            params_dict[name] = theta_flat_prime[:, start_idx:start_idx + param_length].view(theta_flat_prime.shape[0], *param.shape)\n",
    "            \n",
    "            start_idx += param_length\n",
    "\n",
    "        def modified_forward(x, idx):\n",
    "            # directly pass the weights\n",
    "            x = F.linear(x, \n",
    "                        weight=params_dict['nn.1.weight'][idx],\n",
    "                        bias=params_dict['nn.1.bias'][idx])\n",
    "            x = torch.relu(x)\n",
    "            x = F.linear(x,\n",
    "                        weight=params_dict['nn.3.weight'][idx],\n",
    "                        bias=params_dict['nn.3.bias'][idx])\n",
    "            x = torch.relu(x)\n",
    "            x = F.linear(x,\n",
    "                        weight=params_dict['nn.5.weight'][idx],\n",
    "                        bias=params_dict['nn.5.bias'][idx])\n",
    "            return x\n",
    "        \n",
    "        \n",
    "        outputs = [modified_forward(inputs, idx) for idx, inputs in enumerate(trainloader)]\n",
    "        # reassign the weights for the next iteration\n",
    "        for i, model in enumerate(batch):\n",
    "            for name, param in model.named_parameters():\n",
    "                param.data = params_dict[name][i].data\n",
    "\n",
    "        loss = torch.stack([criterion(output, labels) for output, labels in zip(outputs, trainloader)])\n",
    "        losses_meta[epoch+1].extend(loss.clone().detach().flatten().squeeze().cpu().numpy().tolist())\n",
    "        loss = loss.mean()\n",
    "        running_loss += loss.item() * len(batch)\n",
    "    print(f'Epoch {epoch}, eval Train Loss {running_loss/len(optimizees)}')\n",
    "\n",
    "\n",
    "losses_adam = [[] for _ in range(epochs+1)]\n",
    "losses_adam[0] = initial_losses\n",
    "for model in optimizees:\n",
    "    model.to('cuda')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to('cuda')\n",
    "            labels = labels.to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        losses_adam[epoch+1].append(running_loss/len(trainloader))\n",
    "    print(f'Epoch {epoch}, Adam Train Loss {running_loss/len(trainloader)}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
