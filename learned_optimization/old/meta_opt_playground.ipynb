{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BaseModel, self).__init__()\n",
    "        seed = torch.randint(0, 1000, (1,)).item()\n",
    "        torch.manual_seed(seed)\n",
    "        # self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.nn(x)\n",
    "    \n",
    "class MetaModel(nn.Module):\n",
    "    def __init__(self, meta_in_dim=8, meta_hidden_dim=16):\n",
    "        super(MetaModel, self).__init__()\n",
    "        self.meta_in_dim = meta_in_dim\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(meta_in_dim+1, meta_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(meta_hidden_dim, meta_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(meta_hidden_dim, meta_in_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, flattened_weights):\n",
    "        outs = []\n",
    "        if len(flattened_weights.shape)==1:\n",
    "            flattened_weights = flattened_weights.unsqueeze(0)\n",
    "\n",
    "        segment_length = self.meta_in_dim\n",
    "        total_segments = flattened_weights.shape[1]//self.meta_in_dim\n",
    "        for i in range(0, flattened_weights.shape[1], segment_length):\n",
    "            x = flattened_weights[:,i:i+self.meta_in_dim]\n",
    "            segment_number = i//segment_length\n",
    "            segment_feat = torch.tensor([((segment_number/total_segments)-.5)*2]).repeat(x.shape[0], 1).to(DEVICE)\n",
    "            x = torch.cat([x, segment_feat], dim=1)\n",
    "            # print(\"X shape\", x.shape)   \n",
    "            outs.append(self.nn(x))\n",
    "        return torch.cat(outs, dim=1)\n",
    "\n",
    "    \n",
    "def init_weights(module, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "\n",
    "def modified_forward(x, seq, idx, params_dict):\n",
    "    for i,layer in enumerate(seq):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            x = F.linear(x,\n",
    "                        weight=params_dict[f'nn.{i}.weight'][idx],\n",
    "                        bias=params_dict[f'nn.{i}.bias'][idx])\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            x = torch.relu(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([800, 1]),\n",
       " torch.Size([800, 1]),\n",
       " torch.Size([200, 1]),\n",
       " torch.Size([200, 1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input_dim = 1\n",
    "hidden_dim = 32\n",
    "output_dim = 1\n",
    "d = sum(p.numel() for p in BaseModel(input_dim, hidden_dim, output_dim).parameters()) +1\n",
    "\n",
    "n=1000\n",
    "X = torch.rand(n, input_dim)*4 -2\n",
    "Y = torch.tanh(X)\n",
    "# shuffle\n",
    "perm = torch.randperm(n)\n",
    "X = X[perm]\n",
    "Y = Y[perm]\n",
    "# random split\n",
    "X_train, X_test = X[:int(n*0.8)], X[int(n*0.8):]\n",
    "Y_train, Y_test = Y[:int(n*0.8)], Y[int(n*0.8):]\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss 1.0710959434509277\n",
      "Epoch 1, Train Loss 0.893869161605835\n",
      "Epoch 2, Train Loss 0.7391670346260071\n",
      "Epoch 3, Train Loss 0.6038690209388733\n",
      "Epoch 4, Train Loss 0.4780109226703644\n",
      "Epoch 5, Train Loss 0.35249099135398865\n",
      "Epoch 6, Train Loss 0.22010397911071777\n",
      "Epoch 7, Train Loss 0.10554607957601547\n",
      "Epoch 8, Train Loss 0.060339707881212234\n",
      "Epoch 9, Train Loss 0.10264185070991516\n",
      "Epoch 10, Train Loss 0.16317462921142578\n",
      "Epoch 11, Train Loss 0.12247287482023239\n",
      "Epoch 12, Train Loss 0.04785441234707832\n",
      "Epoch 13, Train Loss 0.05460574850440025\n",
      "Epoch 14, Train Loss 0.11274286359548569\n"
     ]
    }
   ],
   "source": [
    "# 1 model\n",
    "theta_f = BaseModel(input_dim, hidden_dim, output_dim).to(DEVICE)\n",
    "meta_model = MetaModel(meta_in_dim=7, meta_hidden_dim=16).to(DEVICE)\n",
    "optimizer = optim.Adam(meta_model.parameters(), lr=1e-2)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "X_train = X_train.to(DEVICE)\n",
    "Y_train = Y_train.to(DEVICE)\n",
    "\n",
    "for epoch in range(15):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # what if we reinit on each iteration? ie can meta model predict final weights from scratch\n",
    "    # theta_f = BaseModel(input_dim, hidden_dim, output_dim)  \n",
    "    \n",
    "    theta_flat = torch.cat([p.flatten() for p in theta_f.parameters()]).requires_grad_(True)\n",
    "    theta_flat = torch.cat([theta_flat, torch.tensor([epoch], dtype=torch.float32,).to(DEVICE)]).requires_grad_(True)\n",
    "    \n",
    "    theta_flat_prime = meta_model(theta_flat).squeeze(0)\n",
    "    \n",
    "    theta_f_prime = BaseModel(input_dim, hidden_dim, output_dim)\n",
    "    \n",
    "    params_dict = {}\n",
    "    start_idx = 0\n",
    "    for name, param in theta_f_prime.named_parameters():\n",
    "        param_length = param.numel()\n",
    "        params_dict[name] = theta_flat_prime[start_idx:start_idx + param_length].view(1,*param.shape)\n",
    "        start_idx += param_length\n",
    "\n",
    "    outputs = modified_forward(X_train, theta_f_prime.nn, 0, params_dict)\n",
    "    loss = criterion(outputs, Y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch}, Train Loss {loss.item()}')\n",
    "    \n",
    "    if epoch == 0:\n",
    "        make_dot(loss, params=dict(list(meta_model.named_parameters()))).render('comp_graph', format='png')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizee params: 97, Meta model params: 519\n"
     ]
    }
   ],
   "source": [
    "# numel\n",
    "optimizee_params = sum(p.numel() for p in BaseModel(input_dim, hidden_dim, output_dim).parameters())\n",
    "meta_model_params = sum(p.numel() for p in meta_model.parameters())\n",
    "\n",
    "print(f\"Optimizee params: {optimizee_params}, Meta model params: {meta_model_params}\") # currently meta_model >> optimizee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss 0.063697474014014\n",
      "Epoch 1, Train Loss 0.031823301747441295\n",
      "Epoch 2, Train Loss 0.009406756018288434\n",
      "Epoch 3, Train Loss 0.005175284341908991\n",
      "Epoch 4, Train Loss 0.003186521677300334\n",
      "Epoch 5, Train Loss 0.002257615766953677\n",
      "Epoch 6, Train Loss 0.001552983481436968\n",
      "Epoch 7, Train Loss 0.001157976841321215\n",
      "Epoch 8, Train Loss 0.0008096691267564893\n",
      "Epoch 9, Train Loss 0.0006756183360703289\n",
      "Epoch 10, Train Loss 0.0006539915823377669\n",
      "Epoch 11, Train Loss 0.0005257102355826646\n",
      "Epoch 12, Train Loss 0.000503846971783787\n",
      "Epoch 13, Train Loss 0.00046089459490031005\n",
      "Epoch 14, Train Loss 0.00041583710466511545\n",
      "Epoch 15, Train Loss 0.00042787148407660424\n",
      "Epoch 16, Train Loss 0.0003831753011327237\n",
      "Epoch 17, Train Loss 0.00038321800786070525\n",
      "Epoch 18, Train Loss 0.0003663084476720542\n",
      "Epoch 19, Train Loss 0.00034297716058790686\n",
      "Epoch 20, Train Loss 0.00034826450143009426\n",
      "Epoch 21, Train Loss 0.00032680845563299953\n",
      "Epoch 22, Train Loss 0.00032551399921067057\n",
      "Epoch 23, Train Loss 0.0003202601292869076\n",
      "Epoch 24, Train Loss 0.005502406366635114\n"
     ]
    }
   ],
   "source": [
    "#k models\n",
    "k = 1000\n",
    "batch_size = 16\n",
    "optimizees = [BaseModel(input_dim, hidden_dim, output_dim) for _ in range(k)]\n",
    "meta_model = MetaModel(7,16).to(DEVICE)\n",
    "optimizer = optim.Adam(meta_model.parameters(), lr=1e-2)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "X_train = X_train.to(DEVICE)\n",
    "Y_train = Y_train.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(25):\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, k, batch_size):\n",
    "        batch = optimizees[i:i+batch_size]\n",
    "        batch_flattened = [torch.cat([torch.cat([p.flatten() for p in model.parameters()]), torch.tensor([epoch], dtype=torch.float32)]) for model in batch]\n",
    "        batch_flattened = torch.stack(batch_flattened).to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        theta_flat_prime = meta_model(batch_flattened)\n",
    "        \n",
    "        batch_prime = [BaseModel(input_dim, hidden_dim, output_dim) for _ in range(theta_flat_prime.shape[0])]\n",
    "        params_dict = {}\n",
    "        start_idx = 0\n",
    "        for name, param in batch_prime[0].named_parameters():\n",
    "            param_length = param.numel() \n",
    "            try:\n",
    "                params_dict[name] = theta_flat_prime[:, start_idx:start_idx + param_length].view(theta_flat_prime.shape[0], *param.shape)\n",
    "            except:\n",
    "                print(name, param_length, theta_flat_prime[:, start_idx:start_idx + param_length].shape)\n",
    "                break\n",
    "            start_idx += param_length\n",
    "\n",
    "        # outputs = [modified_forward(X_train, i) for i in range(len(batch_prime))]\n",
    "        outputs = [modified_forward(X_train, batch_prime[i].nn, i, params_dict) for i in range(len(batch_prime))]\n",
    "        # reassign the outputs to the models\n",
    "        for i, model in enumerate(optimizees[i:i+batch_size]):\n",
    "            for name, param in model.named_parameters():\n",
    "                param.data = params_dict[name][i].cpu()\n",
    "        loss = torch.stack([criterion(output, Y_train) for output in outputs]).mean()\n",
    "        loss.backward()\n",
    "        running_loss += loss.item() * len(batch)\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch}, Train Loss {running_loss/len(optimizees)}')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Train Loss 0.5619182939827442\n",
      "Epoch 0, eval Train Loss 0.1667139571905136\n",
      "Epoch 1, eval Train Loss 0.17691565096378326\n",
      "Epoch 2, eval Train Loss 0.17047459602355958\n",
      "Epoch 3, eval Train Loss 0.15932077050209045\n",
      "Epoch 4, eval Train Loss 0.1468774390220642\n",
      "Epoch 5, eval Train Loss 0.1339113509654999\n",
      "Epoch 6, eval Train Loss 0.12152621656656265\n",
      "Epoch 7, eval Train Loss 0.10974109172821045\n",
      "Epoch 8, eval Train Loss 0.09838734477758408\n",
      "Epoch 9, eval Train Loss 0.08709530532360077\n",
      "Epoch 10, eval Train Loss 0.07579180896282196\n",
      "Epoch 11, eval Train Loss 0.0652143806219101\n",
      "Epoch 12, eval Train Loss 0.05542971178889275\n",
      "Epoch 13, eval Train Loss 0.046443227231502536\n",
      "Epoch 14, eval Train Loss 0.038255318999290466\n",
      "Epoch 15, eval Train Loss 0.03086600348353386\n",
      "Epoch 16, eval Train Loss 0.024275329113006592\n",
      "Epoch 17, eval Train Loss 0.018483226895332338\n",
      "Epoch 18, eval Train Loss 0.013489736989140511\n",
      "Epoch 19, eval Train Loss 0.009412897899746895\n",
      "Epoch 20, eval Train Loss 0.006100185420364141\n",
      "Epoch 21, eval Train Loss 0.003530666958540678\n",
      "Epoch 22, eval Train Loss 0.0017036813497543334\n",
      "Epoch 23, eval Train Loss 0.0006191954179666936\n",
      "Epoch 24, eval Train Loss 0.0002772129140794277\n",
      "Epoch 24, Adam Train Loss 0.0034336368553340435\n",
      "Epoch 24, Adam Train Loss 0.01361022237688303\n",
      "Epoch 24, Adam Train Loss 0.013375019654631615\n",
      "Epoch 24, Adam Train Loss 0.013164897449314594\n",
      "Epoch 24, Adam Train Loss 0.0103409793227911\n",
      "Epoch 24, Adam Train Loss 0.027933716773986816\n",
      "Epoch 24, Adam Train Loss 0.008578799664974213\n",
      "Epoch 24, Adam Train Loss 0.011788041330873966\n",
      "Epoch 24, Adam Train Loss 0.007555740885436535\n",
      "Epoch 24, Adam Train Loss 0.016508420929312706\n",
      "Epoch 24, Adam Train Loss 0.028514422476291656\n",
      "Epoch 24, Adam Train Loss 0.01833217963576317\n",
      "Epoch 24, Adam Train Loss 0.009602047502994537\n",
      "Epoch 24, Adam Train Loss 0.012121675536036491\n",
      "Epoch 24, Adam Train Loss 0.012560505419969559\n",
      "Epoch 24, Adam Train Loss 0.025662340223789215\n",
      "Epoch 24, Adam Train Loss 0.005131666082888842\n",
      "Epoch 24, Adam Train Loss 0.02362910285592079\n",
      "Epoch 24, Adam Train Loss 0.01875903084874153\n",
      "Epoch 24, Adam Train Loss 0.011739935725927353\n",
      "Epoch 24, Adam Train Loss 0.01769796945154667\n",
      "Epoch 24, Adam Train Loss 0.002975830342620611\n",
      "Epoch 24, Adam Train Loss 0.018539192155003548\n",
      "Epoch 24, Adam Train Loss 0.0034336368553340435\n",
      "Epoch 24, Adam Train Loss 0.01361022237688303\n",
      "Epoch 24, Adam Train Loss 0.013375019654631615\n",
      "Epoch 24, Adam Train Loss 0.013164897449314594\n",
      "Epoch 24, Adam Train Loss 0.0103409793227911\n",
      "Epoch 24, Adam Train Loss 0.027933716773986816\n",
      "Epoch 24, Adam Train Loss 0.008578799664974213\n",
      "Epoch 24, Adam Train Loss 0.011788041330873966\n",
      "Epoch 24, Adam Train Loss 0.007555740885436535\n",
      "Epoch 24, Adam Train Loss 0.016508420929312706\n",
      "Epoch 24, Adam Train Loss 0.028514422476291656\n",
      "Epoch 24, Adam Train Loss 0.01833217963576317\n",
      "Epoch 24, Adam Train Loss 0.009602047502994537\n",
      "Epoch 24, Adam Train Loss 0.012121675536036491\n",
      "Epoch 24, Adam Train Loss 0.012560505419969559\n",
      "Epoch 24, Adam Train Loss 0.025662340223789215\n",
      "Epoch 24, Adam Train Loss 0.005131666082888842\n",
      "Epoch 24, Adam Train Loss 0.02362910285592079\n",
      "Epoch 24, Adam Train Loss 0.01875903084874153\n",
      "Epoch 24, Adam Train Loss 0.011739935725927353\n",
      "Epoch 24, Adam Train Loss 0.01769796945154667\n",
      "Epoch 24, Adam Train Loss 0.002975830342620611\n",
      "Epoch 24, Adam Train Loss 0.018539192155003548\n",
      "Epoch 24, Adam Train Loss 0.0034336368553340435\n",
      "Epoch 24, Adam Train Loss 0.01361022237688303\n",
      "Epoch 24, Adam Train Loss 0.013375019654631615\n",
      "Epoch 24, Adam Train Loss 0.013164897449314594\n",
      "Epoch 24, Adam Train Loss 0.0103409793227911\n",
      "Epoch 24, Adam Train Loss 0.027933716773986816\n",
      "Epoch 24, Adam Train Loss 0.008578799664974213\n",
      "Epoch 24, Adam Train Loss 0.011788041330873966\n",
      "Epoch 24, Adam Train Loss 0.007555740885436535\n",
      "Epoch 24, Adam Train Loss 0.016508420929312706\n",
      "Epoch 24, Adam Train Loss 0.028514422476291656\n",
      "Epoch 24, Adam Train Loss 0.01833217963576317\n",
      "Epoch 24, Adam Train Loss 0.009602047502994537\n",
      "Epoch 24, Adam Train Loss 0.012121675536036491\n",
      "Epoch 24, Adam Train Loss 0.012560505419969559\n",
      "Epoch 24, Adam Train Loss 0.025662340223789215\n",
      "Epoch 24, Adam Train Loss 0.005131666082888842\n",
      "Epoch 24, Adam Train Loss 0.02362910285592079\n",
      "Epoch 24, Adam Train Loss 0.01875903084874153\n",
      "Epoch 24, Adam Train Loss 0.011739935725927353\n",
      "Epoch 24, Adam Train Loss 0.01769796945154667\n",
      "Epoch 24, Adam Train Loss 0.002975830342620611\n",
      "Epoch 24, Adam Train Loss 0.018539192155003548\n",
      "Epoch 24, Adam Train Loss 0.0034336368553340435\n",
      "Epoch 24, Adam Train Loss 0.01361022237688303\n",
      "Epoch 24, Adam Train Loss 0.013375019654631615\n",
      "Epoch 24, Adam Train Loss 0.013164897449314594\n",
      "Epoch 24, Adam Train Loss 0.0103409793227911\n",
      "Epoch 24, Adam Train Loss 0.027933716773986816\n",
      "Epoch 24, Adam Train Loss 0.008578799664974213\n",
      "Epoch 24, Adam Train Loss 0.011788041330873966\n",
      "Epoch 24, Adam Train Loss 0.007555740885436535\n",
      "Epoch 24, Adam Train Loss 0.016508420929312706\n",
      "Epoch 24, Adam Train Loss 0.028514422476291656\n",
      "Epoch 24, Adam Train Loss 0.01833217963576317\n",
      "Epoch 24, Adam Train Loss 0.009602047502994537\n",
      "Epoch 24, Adam Train Loss 0.012121675536036491\n",
      "Epoch 24, Adam Train Loss 0.012560505419969559\n",
      "Epoch 24, Adam Train Loss 0.025662340223789215\n",
      "Epoch 24, Adam Train Loss 0.005131666082888842\n",
      "Epoch 24, Adam Train Loss 0.02362910285592079\n",
      "Epoch 24, Adam Train Loss 0.01875903084874153\n",
      "Epoch 24, Adam Train Loss 0.011739935725927353\n",
      "Epoch 24, Adam Train Loss 0.01769796945154667\n",
      "Epoch 24, Adam Train Loss 0.002975830342620611\n",
      "Epoch 24, Adam Train Loss 0.018539192155003548\n",
      "Epoch 24, Adam Train Loss 0.0034336368553340435\n",
      "Epoch 24, Adam Train Loss 0.01361022237688303\n",
      "Epoch 24, Adam Train Loss 0.013375019654631615\n",
      "Epoch 24, Adam Train Loss 0.013164897449314594\n",
      "Epoch 24, Adam Train Loss 0.0103409793227911\n",
      "Epoch 24, Adam Train Loss 0.027933716773986816\n",
      "Epoch 24, Adam Train Loss 0.008578799664974213\n",
      "Epoch 24, Adam Train Loss 0.011788041330873966\n"
     ]
    }
   ],
   "source": [
    "# evaluate overparameterized meta optimizer\n",
    "k_test = 100\n",
    "batch_size = 8\n",
    "optimizees = [BaseModel(input_dim, hidden_dim, output_dim) for _ in range(k_test)]\n",
    "optimizees_for_adam = [deepcopy(model) for model in optimizees]\n",
    "meta_model.to(DEVICE)\n",
    "meta_model.eval()\n",
    "X_train = X_train.to(DEVICE)\n",
    "Y_train = Y_train.to(DEVICE)\n",
    "\n",
    "# initial evaluation on train\n",
    "initial_losses = [] \n",
    "with torch.no_grad():\n",
    "    for model in optimizees:\n",
    "        model.to(DEVICE)\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, Y_train)\n",
    "        initial_losses.append(loss.item())\n",
    "    print(f'Initial Train Loss {sum(initial_losses)/len(optimizees)}')\n",
    "\n",
    "epochs = 25\n",
    "losses_meta = [[] for _ in range(epochs+1)]\n",
    "losses_meta[0] = initial_losses\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, k_test, batch_size):\n",
    "        batch = optimizees[i:i+batch_size]\n",
    "        batch_flattened = [torch.cat([torch.cat([p.flatten() for p in model.parameters()]), torch.tensor([epoch], dtype=torch.float32).to(DEVICE)]) for model in batch]\n",
    "        batch_flattened = torch.stack(batch_flattened).to(DEVICE)\n",
    "\n",
    "        theta_flat_prime = meta_model(batch_flattened)\n",
    "        \n",
    "        batch_prime = [BaseModel(input_dim, hidden_dim, output_dim) for _ in range(theta_flat_prime.shape[0])]\n",
    "        params_dict = {}\n",
    "        start_idx = 0\n",
    "        for name, param in batch_prime[0].named_parameters():\n",
    "            param_length = param.numel() \n",
    "            params_dict[name] = theta_flat_prime[:, start_idx:start_idx + param_length].view(theta_flat_prime.shape[0], *param.shape)\n",
    "            \n",
    "            start_idx += param_length\n",
    "\n",
    "        \n",
    "        outputs = [modified_forward(X_train, batch_prime[i].nn, i, params_dict) for i in range(len(batch_prime))]\n",
    "        # reassign the weights for the next iteration\n",
    "        for i, model in enumerate(batch):\n",
    "            for name, param in model.named_parameters():\n",
    "                param.data = params_dict[name][i].data\n",
    "\n",
    "        loss = torch.stack([criterion(output, Y_train) for output in outputs])\n",
    "        losses_meta[epoch+1].extend(loss.clone().detach().flatten().squeeze().cpu().numpy().tolist())\n",
    "        loss = loss.mean()\n",
    "        running_loss += loss.item() * len(batch)\n",
    "    print(f'Epoch {epoch}, eval Train Loss {running_loss/len(optimizees)}')\n",
    "\n",
    "\n",
    "losses_adam = [[] for _ in range(epochs+1)]\n",
    "losses_adam[0] = initial_losses\n",
    "for model in optimizees_for_adam:\n",
    "    model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, Y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses_adam[epoch+1].append(loss.item())\n",
    "    print(f'Epoch {epoch}, Adam Train Loss {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plto\n",
    "avg_losses_meta = [sum(losses)/len(losses) for losses in losses_meta]\n",
    "avg_losses_adam = [sum(losses)/len(losses) for losses in losses_adam]\n",
    "\n",
    "# error bars\n",
    "std_losses_meta = [torch.tensor(losses).std().item() for losses in losses_meta]\n",
    "std_losses_adam = [torch.tensor(losses).std().item() for losses in losses_adam]\n",
    "\n",
    "plt.errorbar(range(epochs+1), avg_losses_meta, yerr=std_losses_meta, label='Meta')\n",
    "plt.errorbar(range(epochs+1), avg_losses_adam, yerr=std_losses_adam, label='Adam')\n",
    "\n",
    "\n",
    "plt.plot(avg_losses_meta, label='Meta')\n",
    "plt.plot(avg_losses_adam, label='Adam')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mnist\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "new_mirror = 'https://ossci-datasets.s3.amazonaws.com/mnist'\n",
    "torchvision.datasets.MNIST.resources = [\n",
    "   ('/'.join([new_mirror, url.split('/')[-1]]), md5)\n",
    "   for url, md5 in torchvision.datasets.MNIST.resources\n",
    "]\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,),\n",
    "     )])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./.data', train=True, download=True, transform=transform)\n",
    "trainset = [(trainset[i][0].flatten(), trainset[i][1]) for i in range(len(trainset))]\n",
    "testset = torchvision.datasets.MNIST(root='./.data', train=False, download=True, transform=transform)   \n",
    "testset = [testset[i][0].flatten() for i in range(len(testset))]\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True,pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST MLP #params:  25450\n"
     ]
    }
   ],
   "source": [
    "class MNIST_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_MLP, self).__init__()\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(28*28, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)\n",
    "    \n",
    "print(\"MNIST MLP #params: \", sum(p.numel() for p in MNIST_MLP().parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss 0.418510689808925\n",
      "Epoch 1, Train Loss 0.2677950159450372\n",
      "Epoch 2, Train Loss 0.21923765747348467\n",
      "Epoch 3, Train Loss 0.18910806164443492\n",
      "Epoch 4, Train Loss 0.1710364762460192\n"
     ]
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True,pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = MNIST_MLP().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch}, Train Loss {running_loss/len(trainloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "821.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25451/31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimzee # params 25450\n",
      "Meta model # params 1105205\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimzee # params\", sum(p.numel() for p in MNIST_MLP().parameters()))\n",
    "print(\"Meta model # params\", sum(p.numel() for p in MetaModel(821, 512).parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch 1\n",
      "starting batch 2\n",
      "starting batch 3\n",
      "starting batch 4\n",
      "starting batch 5\n",
      "starting batch 6\n",
      "starting batch 7\n",
      "Epoch 0, meta Train Loss 2.7210097312927246\n",
      "starting batch 1\n",
      "starting batch 2\n",
      "starting batch 3\n",
      "starting batch 4\n",
      "starting batch 5\n",
      "starting batch 6\n",
      "starting batch 7\n",
      "Epoch 1, meta Train Loss 2.319272756576538\n",
      "starting batch 1\n",
      "starting batch 2\n",
      "starting batch 3\n",
      "starting batch 4\n",
      "starting batch 5\n",
      "starting batch 6\n",
      "starting batch 7\n",
      "Epoch 2, meta Train Loss 2.303067922592163\n",
      "starting batch 1\n",
      "starting batch 2\n",
      "starting batch 3\n",
      "starting batch 4\n",
      "starting batch 5\n",
      "starting batch 6\n",
      "starting batch 7\n",
      "Epoch 3, meta Train Loss 2.304128646850586\n",
      "starting batch 1\n",
      "starting batch 2\n",
      "starting batch 3\n",
      "starting batch 4\n",
      "starting batch 5\n",
      "starting batch 6\n",
      "starting batch 7\n",
      "Epoch 4, meta Train Loss 2.3047678470611572\n",
      "starting batch 1\n",
      "starting batch 2\n",
      "starting batch 3\n",
      "starting batch 4\n",
      "starting batch 5\n",
      "starting batch 6\n",
      "starting batch 7\n",
      "Epoch 5, meta Train Loss 2.3047237396240234\n",
      "starting batch 1\n",
      "starting batch 2\n",
      "starting batch 3\n",
      "starting batch 4\n",
      "starting batch 5\n",
      "starting batch 6\n",
      "starting batch 7\n",
      "Epoch 6, meta Train Loss 2.3032119274139404\n",
      "starting batch 1\n",
      "starting batch 2\n",
      "starting batch 3\n",
      "starting batch 4\n",
      "starting batch 5\n",
      "starting batch 6\n",
      "starting batch 7\n",
      "Epoch 7, meta Train Loss 2.3024332523345947\n",
      "starting batch 1\n",
      "starting batch 2\n",
      "starting batch 3\n",
      "starting batch 4\n",
      "starting batch 5\n",
      "starting batch 6\n",
      "starting batch 7\n",
      "Epoch 8, meta Train Loss 2.3014838695526123\n",
      "starting batch 1\n",
      "starting batch 2\n",
      "starting batch 3\n",
      "starting batch 4\n",
      "starting batch 5\n",
      "starting batch 6\n",
      "starting batch 7\n",
      "Epoch 9, meta Train Loss 2.301727294921875\n",
      "starting batch 1\n",
      "starting batch 2\n",
      "starting batch 3\n",
      "starting batch 4\n",
      "starting batch 5\n",
      "starting batch 6\n",
      "starting batch 7\n",
      "Epoch 10, meta Train Loss 2.3013455867767334\n",
      "starting batch 1\n",
      "starting batch 2\n",
      "starting batch 3\n",
      "starting batch 4\n",
      "starting batch 5\n",
      "starting batch 6\n",
      "starting batch 7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m batch_flattened \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(batch_flattened)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m theta_flat_prime \u001b[38;5;241m=\u001b[39m \u001b[43mmeta_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_flattened\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m mnist_nn \u001b[38;5;241m=\u001b[39m MNIST_MLP()\n\u001b[1;32m     44\u001b[0m mnist_seq \u001b[38;5;241m=\u001b[39m mnist_nn\u001b[38;5;241m.\u001b[39mnn\n",
      "File \u001b[0;32m~/learned-optimizers/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/learned-optimizers/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[15], line 43\u001b[0m, in \u001b[0;36mMetaModel.forward\u001b[0;34m(self, flattened_weights)\u001b[0m\n\u001b[1;32m     41\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, segment_feat], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# print(\"X shape\", x.shape)   \u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     outs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(outs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/learned-optimizers/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/learned-optimizers/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/learned-optimizers/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/learned-optimizers/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/learned-optimizers/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/learned-optimizers/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "batch_size = 16\n",
    "optimizees = [MNIST_MLP().to(DEVICE) for _ in range(k)]\n",
    "\n",
    "meta_model = MetaModel(meta_in_dim=31, meta_hidden_dim=64).to(DEVICE)\n",
    "optimizer = optim.Adam(meta_model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True,pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "# initial_losses = []\n",
    "# with torch.no_grad():\n",
    "#     for model in optimizees:\n",
    "#         model.to(DEVICE)\n",
    "#         model.eval()\n",
    "#         running_loss = 0.0\n",
    "#         for i, data in enumerate(trainloader):\n",
    "#             inputs, labels = data\n",
    "#             inputs = inputs.to(DEVICE)\n",
    "#             labels = labels.to(DEVICE)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             running_loss += loss.item()\n",
    "#         initial_losses.append(running_loss/len(trainloader))\n",
    "#     print(f'Initial Train Loss {sum(initial_losses)/len(optimizees)}')\n",
    "\n",
    "epochs = 250\n",
    "losses_meta = [[] for _ in range(epochs+1)]\n",
    "initial_losses = 0\n",
    "losses_meta[0] = initial_losses\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, k, batch_size):\n",
    "        print(\"starting batch\", (i//batch_size)+1)\n",
    "        batch = optimizees[i:i+batch_size]\n",
    "        batch_flattened = [torch.cat([torch.cat([p.flatten() for p in model.parameters()]), torch.tensor([epoch], dtype=torch.float32).to(DEVICE)]) for model in batch]\n",
    "        batch_flattened = torch.stack(batch_flattened).to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        theta_flat_prime = meta_model(batch_flattened)\n",
    "        \n",
    "        mnist_nn = MNIST_MLP()\n",
    "        mnist_seq = mnist_nn.nn\n",
    "        params_dict = {}\n",
    "        start_idx = 0\n",
    "        for name, param in mnist_nn.named_parameters():\n",
    "            param_length = param.numel() \n",
    "            params_dict[name] = theta_flat_prime[:, start_idx:start_idx + param_length].view(theta_flat_prime.shape[0], *param.shape)\n",
    "            \n",
    "            start_idx += param_length\n",
    "        \n",
    "        # shuffle trainloader\n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True,pin_memory=True)\n",
    "        it = iter(trainloader)\n",
    "        outputs = []\n",
    "        labels = []\n",
    "        for i in range(theta_flat_prime.shape[0]):\n",
    "            inputs, label = next(it)\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            label = label.to(DEVICE) \n",
    "            out = modified_forward(inputs, mnist_seq, i, params_dict)\n",
    "            loss = criterion(out, label)\n",
    "            retain_graph = i < theta_flat_prime.shape[0] - 1\n",
    "            loss.backward(retain_graph=retain_graph)\n",
    "            losses_meta[epoch+1].append(loss.item())    \n",
    "        # reassign the weights for the next iteration\n",
    "        for i, model in enumerate(batch):\n",
    "            for name, param in model.named_parameters():\n",
    "                param.data = params_dict[name][i].data\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch}, meta Train Loss {torch.mean(torch.tensor(losses_meta[epoch+1]))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
